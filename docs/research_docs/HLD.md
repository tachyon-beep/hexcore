# Introduction

Magic: The Gathering (MTG) is a highly complex strategy card game with thousands of unique cards and intricate rules. Developing a large language model (LLM) to _master_ MTG requires careful planning across multiple domains: game state analysis, deck construction, rules comprehension, advanced reasoning, and dynamic knowledge integration. This report presents a comprehensive analysis and design plan for training an LLM to excel in MTG through the following capabilities:

- **Analyse Plays** – Evaluating board states and recommending optimal moves.
- **Assemble Decks** – Suggesting deck builds based on archetypes, meta, and playstyle.
- **Answer Rules Questions** – Providing authoritative answers grounded in official rules and card texts.
- **Advanced Reasoning** – Utilizing five reasoning modes (REASON, EXPLAIN, TEACH, PREDICT, RETROSPECT) to enhance decision-making and explanation.
- **Leverage KG/RAG** – Integrating a Knowledge Graph (KG) and/or Retrieval-Augmented Generation (RAG) to supply factual game knowledge and dynamically invoke these resources when needed.

Each aspect is addressed in detail, from model selection and training strategies to system architecture and project planning. The goal is to outline how to train and deploy an **MTG-savvy LLM** that can act as a play advisor, deckbuilder, rules guru, and insightful tutor for players.

## Model Selection

Choosing the right base model is critical for handling MTG’s complexity. We compare **Mixtral 8x7B** (a Mixture-of-Experts model by Mistral AI) with alternative open-source LLMs, focusing on reasoning capability, efficiency, and architecture:

- **Mixtral 8x7B (Mistral)**: This model uses a **sparse Mixture-of-Experts (MoE)** architecture: 8 expert sub-models (~7B parameters each) with a router that activates only 2 experts per token ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=The%20image%20below%20illustrates%20a,together%20in%20an%20additive%20manner)). Effectively, each token is processed by ~13B parameters (2×7B), while total parameters (~40–50B) provide a rich knowledge capacity ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=So%2C%20why%20use%20MoEs%3F%20In,50B)) ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=Mixtral%208x7B%20%2F%20Mistral%207B,LLaMa)). This yields _performance comparable to much larger models_ (it outperforms Meta’s Llama-2 70B on most benchmarks, and even matches or exceeds GPT-3.5 on many tasks ([Mistral AI's Mixtral-8x7B: Performance](https://arize.com/blog/mistral-ai/#:~:text=Mixtral,at%20DeepMind%20and%20Google%20Research))) but with far lower computation per token. The router ensures only a fraction of weights are active at a time, making inference ~6× faster than using a full 70B model ([Mistral AI's Mixtral-8x7B: Performance](https://arize.com/blog/mistral-ai/#:~:text=Mixtral,at%20DeepMind%20and%20Google%20Research)). Mixtral also supports a **32k token context window** ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=%2A%20It%20handles%2032k,Bench)) – crucial for MTG where long rule texts or multiple card details might need to be considered at once. These advantages make Mixtral 8x7B a strong candidate for complex reasoning in MTG. However, its MoE design means the full model is larger to store in memory (all 8 experts ~40B+ parameters need to be loaded). Fine-tuning MoE models can be more complex but is supported by libraries (Flash Attention 2, bitsandbytes, PEFT) ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=The%20model%20is%20compatible%20with,on%20the%20Hugging%20Face%20Hub)).

- **LLaMA-2 and Derivatives**: LLaMA-2 is a family of dense decoder-only models (7B, 13B, 70B) from Meta. The 70B variant is a top-performing open model for reasoning and knowledge, but it requires significantly more resources (all 70B parameters active per token). In contrast, Mixtral achieves similar or better results with only ~13B active parameters ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=Mixtral%208x7B%20%2F%20Mistral%207B,LLaMa)). Smaller LLaMA-2 models (13B or 7B) would be easier to fine-tune but may struggle with MTG’s complexity (the gap in reasoning ability between 7B and 70B is large). If resources allow, LLaMA-2 70B (or fine-tuned versions like Code Llama or WizardLM-70B) could be an alternative baseline for strong performance, though with much higher inference cost. The **trade-off** is between **raw power vs. efficiency** – e.g., LLaMA-2-70B might slightly outperform Mixtral on certain tasks, but Mixtral’s _parameter efficiency_ provides near-70B performance at a fraction of runtime cost ([Mistral AI's Mixtral-8x7B: Performance](https://arize.com/blog/mistral-ai/#:~:text=Mixtral,at%20DeepMind%20and%20Google%20Research)).

- **Other Open-Source Models**: Options include **Falcon 40B**, **GPT-NeoX/Pythia**, or newer models like **Mistral 7B (dense)**. Falcon-40B offers good language ability but is not specifically fine-tuned for MTG-like reasoning and lacks the instruct fine-tuning out-of-the-box. Mistral 7B is an improved 7B base model (on par or better than LLaMA-2-7B) and serves as the backbone of Mixtral’s experts. It’s efficient but likely needs heavy fine-tuning to reach the required reasoning depth. **GPT-4 or Claude** (proprietary) are top-tier for reasoning but cannot be fine-tuned or integrated as open solutions. Since we aim for an open, self-hosted model, Mixtral 8x7B stands out as a **sweet spot** – combining the strength of a ~50B model with the speed of a 7B model, and proven strong performance on reasoning benchmarks ([Mistral AI's Mixtral-8x7B: Performance](https://arize.com/blog/mistral-ai/#:~:text=Mixtral,at%20DeepMind%20and%20Google%20Research)). This will allow complex MTG logic without prohibitive hardware requirements.

**Recommendation:** Use **Mixtral 8x7B** as the base model for MTG LLM training. Its high benchmark performance in code and reasoning tasks suggests it can handle MTG’s complexity ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=Mixtral%208x7B%20%2F%20Mistral%207B,LLaMa)). The large context window (32k) is a significant benefit for reading long card lists or rule documents. Alternative open models (like LLaMA-2-70B or Falcon-40B) could be considered if Mixtral integration proves difficult, but they would require more optimization to run. For parameter-efficient fine-tuning, we will keep most of the model weights frozen and apply targeted updates (see Training section). Mixtral’s compatibility with PEFT tools ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=The%20model%20is%20compatible%20with,on%20the%20Hugging%20Face%20Hub)) makes it feasible to fine-tune despite its unique architecture.

## Reasoning Methodologies (CoT, MCTS, R1)

To enable **advanced reasoning** in gameplay and rules analysis, we consider several reasoning paradigms and how to incorporate them:

- **Chain-of-Thought (CoT)**: This involves prompting or training the model to **think step-by-step**, breaking down complex problems into intermediate steps. CoT reasoning has been shown to greatly improve performance on math, logic, and multi-step problems ([DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1#:~:text=align%20with%20social%20values%2C%20and,based%20reward%20models%20%28Uesato)). For our MTG model, CoT might look like the model internally enumerating the board state factors (“<ins>Begin reasoning</ins>: _The opponent has two untapped creatures... my life is 5... possible moves are X, Y._ …”) before giving a recommendation. We will fine-tune the model with CoT style exemplars so it learns to _REASON_ through scenarios. This can be done by including thought processes in training data (possibly delimited by special tokens or markup). CoT is relatively straightforward to implement via supervised fine-tuning: provide question → model’s step-by-step reasoning → final answer. We expect CoT to enhance the model’s “REASON” and “EXPLAIN” capabilities by making its logic explicit. One challenge is ensuring the chain-of-thought remains correct and doesn’t introduce errors; we will encourage _self-consistency_, training the model to verify each step. CoT prompting has minimal overhead (just additional tokens for reasoning) and leverages the model’s own knowledge. It is a core technique for MTG tasks where reasoning about consequences (like predicting opponent moves or interactions) is vital.

- **Monte Carlo Tree Search (MCTS)**: MCTS is a search algorithm used in game-playing AIs (notably AlphaGo) to explore possible moves by simulation. Integrating MCTS with an LLM could mean using the model as an evaluator or move generator in a search tree. For instance, given a board state, the model could simulate different action sequences (play card A, then opponent responds, etc.) and estimate outcomes, effectively performing lookahead. Some recent research has explored combining LLMs with tree search to improve reliability on complex tasks ([](https://arxiv.org/pdf/2406.07394#:~:text=Abstract%20This%20paper%20introduces%20the,Monte%20Carlo%20search%20tree%20through)). However, applying MCTS to MTG via an LLM is challenging due to the enormous branching factor and hidden information. We might instead use a **Tree-of-Thoughts** approach – letting the model generate multiple possible reasoning paths or move sequences, and then evaluating which path leads to the best outcome. This is analogous to MCTS but with the LLM’s internal reasoning steps. For example, the model could propose 2-3 candidate plays, simulate likely opponent responses for a few turns (in a simplified manner), and then choose the play with the best outcome probability. This could be done in a single prompt (with the model writing out and comparing lines of play) or through an iterative tool process. While promising, prior attempts at using search algorithms with LLM reasoning have had mixed results – none have yet matched the general reasoning performance of top CoT-based models like OpenAI’s _o1_ ([DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1#:~:text=explored%20various%20approaches%2C%20including%20process,to%20OpenAI%E2%80%99s%20o1%20series%20models)). MCTS is computationally expensive and would be used at _inference_ time rather than something we directly train into the model (the model itself isn’t performing tree search unless guided to). Our plan is to incorporate **elements of search during training** in a limited way: for example, generating training examples where the model is asked to _PREDICT_ outcomes of different choices, forcing it to consider alternate lines. This will at least familiarize it with the concept of evaluating multiple possibilities. The actual implementation of MCTS could be an external module that queries the model repeatedly (which can be added during inference stage if needed for optimal play recommendations). In summary, MCTS offers _optimality through exploration_ but at the cost of complexity; it may be an enhancement later, whereas CoT and R1 are more immediate training strategies.

- **DeepSeek R1-Style Reasoning**: _DeepSeek-R1_ is a recent approach that uses **reinforcement learning (GRPO)** to heavily fine-tune an LLM for rigorous reasoning ([How DeepSeek-R1 Was Built; For dummies](https://www.vellum.ai/blog/the-training-of-deepseek-r1-and-ways-to-use-it#:~:text=Using%20the%20GRPO%20RL%20framework)). The R1 model reportedly achieved outstanding results on reasoning benchmarks, matching OpenAI’s models by using extremely long chain-of-thoughts and self-refinement ([DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1#:~:text=reasoning%20behaviors,0912)). In essence, R1-style reasoning trains the model to **always produce a reasoned solution with a clear separation between the reasoning process and the final answer**. For example, R1 models output a structured format: a “Thought” section and a “Solution” section demarcated by special tokens ([README.md · mlabonne/dolphin-r1-deepseek at main](https://huggingface.co/datasets/mlabonne/dolphin-r1-deepseek/blob/main/README.md#:~:text=This%20is%20a%20reformatted%20version,end_of_solution)). This method was enabled by massive scale (DeepSeek-R1 has 671B parameters, requiring multi-node GPU setups ([Private, secure DeepSeek-R1 in production in US & EU data centers | Baseten Blog](https://www.baseten.co/blog/private-secure-deepseek-r1-in-production-in-us-eu-data-centers/#:~:text=Multi))) and novel RL fine-tuning techniques. We obviously cannot replicate training a 671B model, but we can adopt the **style and techniques** on a smaller scale. In practice, this means fine-tuning our model on high-quality reasoning datasets (like the _Dolphin_ dataset which emulates DeepSeek-R1 outputs ([README.md · mlabonne/dolphin-r1-deepseek at main](https://huggingface.co/datasets/mlabonne/dolphin-r1-deepseek/blob/main/README.md#:~:text=This%20is%20a%20reformatted%20version,end_of_solution))) so that it learns to think in a structured, rigorous way. The benefit of R1-style training is an improvement in _logical coherence and multi-step problem solving_ without needing human-labeled step-by-step examples for everything – the RL process encourages the model to develop these skills. Since we may not have the resources for full RL training, we will rely on **distillation of R1 behaviors**: incorporating data generated by R1 or similar models into our fine-tuning. Indeed, versions of R1 have been distilled to smaller models with much lower cost, while preserving the reasoning style ([Private, secure DeepSeek-R1 in production in US & EU data centers](https://www.baseten.co/blog/private-secure-deepseek-r1-in-production-in-us-eu-data-centers/#:~:text=centers%20www,style%20reasoning)). We will incorporate special tokens or prompts to trigger modes like REASON, EXPLAIN, etc., similar to R1’s approach of delineating thought and answer. By doing so, our LLM should handle the five reasoning modes effectively:

  - _REASON_: internal chain-of-thought (possibly hidden or in a separate section of output).
  - _EXPLAIN_: articulate reasoning in a user-friendly manner.
  - _TEACH_: guide a learner through concepts (which might involve breaking down rules or strategy in steps).
  - _PREDICT_: forecast game outcomes or opponent moves (requiring simulation of possibilities).
  - _RETROSPECT_: analyze past plays or errors after the fact.

  The training will include prompts explicitly requesting each of these modes to ensure the model can distinguish and perform them. R1-style fine-tuning will reinforce the model’s tendency to engage in deep reasoning before answering, which is crucial for complex MTG questions. It’s essentially an advanced form of CoT with reinforcement backing – since we cannot run full GRPO ourselves, we use supervised fine-tuning with R1-like data and perhaps _reward modeling_ in a smaller scope (e.g., prefer answers that have correct reasoning steps).

**Comparison:** In summary, **CoT** is a must-have baseline for reasoning (easy to implement and effective), **R1-style** adds structured and rigorous reasoning via specialized training (elevating CoT to a new level of reliability), and **MCTS**/search-based reasoning can be an optional enhancement for specific tasks like finding optimal plays by exploring alternatives. We will prioritize CoT and R1 techniques in training to imbue the model with strong reasoning skills. MCTS may be incorporated at inference as a tool if needed, rather than as a learned behavior of the model (though the model will be aware of evaluating multiple lines). This multi-pronged approach ensures the model not only computes answers but justifies and checks them, aligning with the _REASON/EXPLAIN/TEACH_ aspects of the project.

## Training Implementation

Building a model proficient in MTG involves assembling the right training data and fine-tuning process. We outline the **dataset design**, **fine-tuning strategy**, and **reasoning augmentation techniques**:

### Dataset Design and Preparation

We will curate a comprehensive, multi-part dataset covering gameplay scenarios, deck construction, rules knowledge, and the five reasoning modes. Key components:

- **Game State → Optimal Play Data**: To train _Analyse Plays_, we need examples of board states and the optimal or recommended moves. We can source this in several ways:

  - _Expert Annotated Scenarios_: Create or collect textual descriptions of game states (e.g., “Turn 5: You have 6 life, control `Lightning Bolt` in hand, opponent has two 3/3 creatures...”) along with expert reasoning and the best move (“Cast Lightning Bolt on one attacker, then … because …”). These could come from **Magic strategy articles or puzzles** published by the community. For example, many forums and articles present “What’s the Play?” scenarios that we can scrape and include as training data (with permission).
  - _Game Logs_: If available, use logs from MTG Arena or Magic Online, which detail sequences of plays. We could reconstruct certain game states from logs and identify pivotal decisions. However, raw logs are verbose and not in natural language; we’d need to convert them to a descriptive format.
  - _Simulation and Synthesis_: We can use the rules engine (if one exists in software) to simulate random board states and then have a heuristic or an oracle (possibly an existing AI or human) determine a good move. Or leverage GPT-4 to generate hypothetical game states and ask it (or a fine-tuned smaller model) to propose a good move with an explanation. Synthetic generation can greatly expand our dataset with diverse scenarios. We must verify the correctness of such data (using known MTG heuristics or manual review) to avoid reinforcing bad habits.

  Each data point in this set will be structured to encourage reasoning: **State description → [Model REASONING] → Best Move + explanation**. By seeing many examples, the LLM will learn to analyze board context and choose actions, addressing the _PREDICT_ and _EXPLAIN_ reasoning modes in gameplay context.

- **Deckbuilding and Meta Knowledge Data**: For _Assemble Decks_, we need the model to suggest deck lists given an archetype or constraints. Data sources include:

  - _Public Decklists_: Large collections of tournament decklists (from sites like MTGGoldfish, MTGTop8, or Wizards event coverage). These provide examples of what cards constitute a successful deck for each format and archetype. We can encode these either as raw lists or in a summarized form (“A classic _Burn_ deck in Modern format runs 4 Goblin Guide, 4 Lightning Bolt, ... etc.”).
  - _Deck Guides and Analyses_: Articles or forum posts where players discuss deck strategy, card choices, and meta considerations (e.g., “Why card X is good in Y matchup”). These are valuable for training the model to **EXPLAIN** _why_ a deck is built a certain way. For instance, a guide might say _“Against control decks, include more haste creatures to pressure early”_ – the LLM can learn such advice.
  - _Archetype Definitions_: A knowledge base of common archetypes (Aggro, Control, Combo, Midrange, etc.), their gameplans, and typical cards. We can create a structured dataset where input is “Preferred playstyle: aggressive, Format: Standard, Budget: medium” and output is a recommended deck list with an explanation.
  - _Meta Reports_: Data from meta reports (which decks are popular or strong in the current environment) can help the model give **up-to-date recommendations**. For initial training, we might use historical meta snapshots. Post-deployment, using RAG to fetch the latest meta info is preferable (since metas change). But the model should at least be aware of the concept of a shifting meta and sideboard planning for known decks.

  The deckbuilding dataset likely will be a mix of **structured data** (card lists) and **narrative explanation**. We will fine-tune the model to output decks in a consistent format (perhaps Markdown list of cards or a bullet list by card category) followed by rationale paragraphs. This covers both _ASSEMBLE (the deck)_ and _TEACH/EXPLAIN_ (the rationale and how it fits the archetype).

- **Rules Q&A and Card Knowledge**: MTG’s comprehensive rules and card texts form the knowledge foundation for the model to _Answer Rules Questions_. Key data:

  - **MTG Comprehensive Rules**: The official rules document (over 250 pages) contains precise rulings for all game mechanics. We will ingest this text into a knowledge store for retrieval (due to its size), but for fine-tuning, we can use an _excerpt-based approach_: create QA pairs where a question about an interaction or rule is answered with quotes or references to the Comprehensive Rules. For example, “Q: Can you counter an ability with Counterspell? A: No – Counterspell targets spells on the stack, not abilities (see rule 112.7a)”. We can generate hundreds of such Q&As by parsing common points of confusion and their answers from the rules.
  - **Card Texts and Rulings**: A database of all card Oracle texts (official card rules text) and Gatherer rulings (clarifications for specific cards) is essential. We might not fine-tune the model on all card texts (there are 25k+ cards, which is too much to embed in parameters), but we will use this for retrieval. However, we will fine-tune on _how to use card text_ in reasoning. For example, present a scenario: “Opponent controls **Rest in Peace** (text: ‘If a card would be put into a graveyard, exile it instead’). You control **Bloodghast** (text: ‘... returns from graveyard to battlefield whenever a land enters’). Will Bloodghast return if a land drops?” and have the model answer by referencing the card texts and relevant rule (in this case, the replacement effect of Rest in Peace prevents Bloodghast from ever being in the graveyard, so it won’t return). By training on many such card interaction Q&As, the model learns to apply rules to specific card abilities.
  - **Judge FAQs / Forums**: There are community resources like Judge forums or StackExchange where complicated rules interactions are discussed. Scraping these (if license allows) gives real examples of questions and expert answers. These are high-value for fine-tuning, as they show _how to articulate a rules explanation_. The model should learn to provide _authoritative answers_, possibly citing rule numbers or quoting card text for clarity. We must ensure the answers in training are correct per latest rules to avoid instilling outdated or wrong interpretations.

  This portion of the dataset will strengthen the model’s _factual accuracy_ on rules and its ability to quote or reference authoritative sources – a key part of being a rules guru. We will also incorporate the **language of the rules** (specific jargon and templating) so the model becomes comfortable with formal rules language and can translate it to plain explanations when teaching.

- **Reasoning and Tutorial Data**: To cultivate the five reasoning modes (especially _TEACH_ and _RETROSPECT_), we’ll include data where the model is explicitly in a teaching or analytical role beyond just Q&A:

  - _Tutorial Dialogues_: E.g., a conversation where a user asks “How do I evaluate what the opponent might do next?” and the model responds in a didactic way, step-by-step (“Let’s break down the situation...”). We can script dialogues or use existing beginner guides to generate Q&A pairs.
  - _Retrospective Analysis_: Data where a past game or turn is analyzed: e.g., “We lost the last game after keeping a one-land hand; let’s do a retrospective.” The model then goes through what happened and what could be improved. This kind of data might be rarer; we may need to create synthetic examples or pull from coaching content where players review their games.
  - _“Teacher mode” Explainers_: Ensure the model can handle requests like “Explain trample in simple terms” or “Teach me how the stack works.” We’ll include simple explanations targeted at various skill levels, gleaned from wiki articles or by simplifying the comprehensive rules.

  By diversifying the roles the model plays in training data (tutor, analyst, rulebook, deck advisor), we reduce the chance that it only knows how to answer direct questions. It will be practiced in deeper reasoning and multi-turn assistance.

**Data Formatting:** All data will be unified into a conversational/instruction format suited for supervised fine-tuning. We will likely use an _instruction tuning_ approach: each example can be a prompt (user query or situation) and a response (model output), possibly with system messages to set context (like specifying the mode or persona if needed). Special tokens or tags may be used to label reasoning sections (as done in R1’s dataset ([README.md · mlabonne/dolphin-r1-deepseek at main](https://huggingface.co/datasets/mlabonne/dolphin-r1-deepseek/blob/main/README.md#:~:text=This%20is%20a%20reformatted%20version,end_of_solution))), but these might be internal. For instance, in training we might include `<|begin_of_thought|> ... <|end_of_thought|>` around the reasoning that should be hidden from the user, followed by the final answer. This would train the model to separate its reasoning from its final answer – helpful for correctness and for possibly exposing the reasoning on demand (like a “why did you suggest that?” follow-up from the user).

We will be careful to balance the dataset among tasks so the model doesn’t overfit to one mode. A roughly even mix of play analysis, deck suggestions, and rules Q&A (plus some general MTG knowledge chat) will make it a well-rounded assistant. If anything, rules Q&A might need the largest share because factual accuracy there is paramount (and it’s easier to verify and augment with retrieval). We’ll also reserve a portion of data for validation – e.g., hold out some known puzzles and questions to test the model’s performance after training.

### Fine-Tuning Strategies (QLoRA & PEFT)

Fine-tuning a model as large as Mixtral 8x7B (or any large LLM) on domain-specific data is resource-intensive. We will leverage **parameter-efficient fine-tuning (PEFT)** techniques, especially **QLoRA**, to adapt the model without full retraining:

- **QLoRA (Quantized LoRA)**: QLoRA allows us to fine-tune a large model in 4-bit precision while adding small low-rank adaptation matrices ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)). The base model’s weights remain mostly frozen (and quantized for memory savings), and only the LoRA adapter weights are learned. This drastically reduces memory requirements – e.g., QLoRA made it feasible to fine-tune a 65B model on a single 48GB GPU with no loss in performance ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)). In our case, Mixtral’s effective size (~40B total, 7-13B active per token) should be manageable on a modern GPU with QLoRA (we may use a multi-GPU setup if needed for speed, but memory-wise a single high-end GPU could suffice when using 4-bit precision). We will apply QLoRA to inject our MTG-specific knowledge. The adapters will effectively _overlay_ the base model, modifying it to produce MTG-focused outputs. This means we don’t have to modify all the expert weights of Mixtral; instead, LoRA will learn which parts of the network to tweak for, say, understanding card names or using rule-like language. We expect this to significantly speed up training and enable iterative tuning (we can fine-tune, evaluate, and fine-tune again on different data mixes easily).

- **Multi-Stage Fine-Tuning**: Given the varied tasks, a single fine-tuning run might not suffice. We plan a multi-stage approach:

  1. **General Instruction Tuning**: First, ensure the base model is aligned to follow instructions and produce chain-of-thought. Mixtral might already have an instruct tuning (scored 8.3 on MT-Bench when fine-tuned ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=,Bench))). We will further tune it on a small set of general Q&A (could use open instruct datasets or a small portion of our MTG data) to establish the format – e.g., always answer in a helpful manner, how to separate reasoning if needed. This acts like a warm start and “common sense” alignment.
  2. **Domain Fine-Tuning**: Next, train on the bulk of the MTG dataset (gameplay, decks, rules). This can be done by mixing all data, but we might also consider sequential fine-tuning: for example, first train on rules Q&A until the model reliably recalls rules structure, then on gameplay scenarios, etc., or vice versa. We must be cautious to not “forget” earlier tasks (catastrophic forgetting). Techniques like interleaving tasks or multi-task objective will be used. We may also utilize **gradual unfreezing** – initially keep most layers frozen except maybe final layers and LoRA, and later allow a bit more finetuning if needed for certain knowledge. However, QLoRA inherently focuses on adapter training only, which prevents forgetting of base capabilities.
  3. **Reasoning Augmentation**: Incorporate the CoT/R1 style data and possibly perform a second pass of fine-tuning where we emphasize _reasoning correctness_. This could involve filtering or reweighting data: e.g., train the model to produce the correct chain-of-thought by providing some problems with and without reasoning and using a reward (if we do a small RLHF step) or simply extra epochs on reasoning examples. If computationally feasible, a brief **reward modeling or RL fine-tune** could be done: have the model generate answers to some questions, use a heuristic or another model to score the reasoning (for correctness or completeness), and fine-tune the model to maximize that score. However, this is advanced and optional given time/hardware constraints – a well-crafted supervised dataset might suffice.

  Throughout these stages, **evaluation** on a validation set will guide us. We’ll monitor performance separately on play recommendations, deck suggestions, and rules accuracy. If we see one area lagging (say the model suggests illegal moves or incorrect rules), we’ll go back and augment the training data or adjust the curriculum.

- **Adapter Composition**: One interesting aspect of PEFT is the ability to merge or switch adapters. We could train separate LoRA adapters for different skills – for example, one specifically on rules Q&A, another on gameplay – and then _merge_ them or activate them as needed. There’s research on merging LoRAs trained on different tasks to create a multi-skill model. Given our integrated approach, we might not need this, but it is a fallback if multi-task training proves tricky. For instance, if a single model has trouble balancing deckbuilding vs rules (since one is creative, one is factual), we could maintain two LoRA experts and programmatically choose which to apply based on query type. Ideally, though, one model can do it all with unified fine-tuning.

- **Continuous Fine-Tuning & Updates**: After initial deployment, we will likely continue fine-tuning in iterations (this goes into the project plan). Using **adversarial training**, we’ll identify where the model fails – e.g., find scenarios where it recommends a clearly wrong play or misunderstands a rule – and add those cases (with correct outputs) to the training data in the next round. Adversarial data can be generated by having the model play against itself or answer tricky questions and then checking for mistakes. This iterative loop will gradually patch weaknesses. We will also incorporate _new data_ (new card sets, emerging deck strategies, updated rules) through periodic fine-tuning or by updating the retrieval knowledge base (more on that in KG/RAG section). The use of retrieval can actually reduce the need to constantly fine-tune for new factual knowledge, focusing fine-tuning efforts on reasoning skills and interpretation of retrieved info.

### Augmenting Reasoning Capabilities

To push the model’s reasoning to an advanced level (so it truly _masters_ MTG strategy and logic), we employ special techniques:

- **Synthetic Data Generation for Reasoning**: We will generate complex scenarios and questions that require multi-step reasoning using external AI (like GPT-4) or scripts. For example, ask GPT-4 to create a series of increasingly difficult puzzles (with answers) about stack interactions or combat math. These can then fine-tune our model, effectively transferring some of GPT-4’s reasoning prowess in this domain. We must ensure the synthetic answers are correct (we can cross-verify with rules or have human validation for a subset). Synthetic data is especially useful for edge cases that might not appear often in real data (like obscure card combos or complicated layer interactions). By training on these, our model will be more robust when such rare questions arise.

- **In-Context Examples & Few-Shot Prompting**: During fine-tuning, we can train the model to make use of examples in the input. For instance, provide demonstrations of a certain reasoning pattern in the prompt and have the model continue. This way, at inference time, we can supply few-shot examples to guide reasoning without always relying on the model’s learned weights alone. For instance, to encourage _RETROSPECT_ mode, we might include in the prompt: “Example: [Description of a misplay] → [Analysis of that misplay].” and then ask the model to do similar for a new situation. Teaching the model to utilize context effectively is like a meta-skill; it will know how to follow the format if we ever prepend something.

- **Adversarial/Contrastive Training**: We will also fine-tune the model to avoid traps. For rules, we might include _incorrect answers_ as negative examples and correct them. Or train it in a contrastive way: give the model two reasoning paths, one leading to a correct answer and one incorrect, and reward the correct one. Another approach is _self-refinement_: train the model on data where it initially gave a wrong answer, then was shown the correct reasoning. This helps it learn to double-check. For example, an adversarial prompt might trick a less careful model (“If you control no creatures, can you cast Giant Growth?” – a naive model might say “No, it needs a target” which is wrong because you can cast it, it will just fizzle). By including these and providing the correct resolution (“Yes, you can initiate the spell, but it will have no effect as there’s no valid target when it resolves【_rule reference_】.”), we inoculate the model against common misconceptions.

- **Validation of Reasoning**: Taking inspiration from R1, we can encourage the model to _validate its own answers_. For instance, after producing an answer, ask it to **predict** what would happen next or verify with a rule quote. We can simulate a “critic” in training: user asks a question, model answers, then a follow-up user message: “Are you sure? Why?” forcing the model to defend or reconsider. Training the model to handle such follow-ups gracefully will improve its reliability and honesty about uncertainty. It should learn to say “Let me double-check that ruling in the comp rules…” and perhaps engage the retrieval (if available) or its memory to confirm.

Overall, the training implementation is designed to impart factual knowledge, strategic thinking, and teaching ability, all through careful dataset construction and iterative fine-tuning. By using **QLoRA** and other efficient methods, we can do this on feasible hardware, and by **augmenting reasoning** explicitly, we aim for an AI that doesn’t just _know_ MTG but can _think about it like an expert_.

## Knowledge Graph vs. Retrieval-Augmented Generation (KG vs RAG)

MTG is a knowledge-intensive domain: thousands of card texts, a huge rulebook, and constantly evolving strategies. No matter how well we train the base model, it may not memorize every detail or stay up-to-date. Integrating external knowledge via a **Knowledge Graph (KG)** and **Retrieval-Augmented Generation (RAG)** can dramatically enhance the model’s accuracy and flexibility. We propose leveraging both, and crucially, enabling the model (or system) to **dynamically decide** when to use each resource.

### Knowledge Graph for MTG

A Knowledge Graph is a structured representation of information – in this case, MTG entities (cards, mechanics, archetypes, etc.) and their relationships. We can construct a specialized KG for key MTG knowledge:

- **Graph Structure**: Nodes could represent cards, card types, abilities, game concepts, deck archetypes, formats, etc. Edges capture relations like _“Card A counters Card B”_, _“Card X has ability Y”_, _“Card Z is part of Archetype Q”_, or _“Rule R applies to mechanic M”_. For example, a node for the _“Flying”_ mechanic might connect to all cards with Flying; a node for _“Burn Deck”_ archetype might connect to typical burn cards, etc. The KG can also encode static truths: _“Lightning Bolt deals 3 damage”_, _“Black Lotus is banned in Legacy”_, or _“Planeswalkers are subject to the ‘legend rule’”_. This is knowledge that doesn’t change per context and can be reliably looked up.

- **Use Cases**: The KG shines in scenarios requiring **exact or connected knowledge**:

  - _Deck Assembly_: If the model knows you want a “Vampire tribal” deck, it can query the KG for “all vampire creature cards in Standard” and use that list to inform its suggestion. Or find synergy: check what cards in your pool share a creature type or strategy.
  - _Rules Reasoning_: A KG representation of the rules could be tricky (since rules are not easily broken into triple format), but we can encode some as relationships. For instance, a simplified rule KG might have nodes for phases, and edges showing order of phases or which actions are allowed in each phase. Or node “Replacement Effect” linking to cards that cause replacement effects. This might help the model _conceptually_ organize the rules.
  - _Card Interactions_: Knowledge that “Rest in Peace -> any card-to-graveyard effect is replaced” can be an edge between the card and the concept of graveyard exile. If the model queries the KG about those two cards, it might retrieve “Rest in Peace nullifies Bloodghast recursion” as a stored fact (we could pre-enter known common interactions into the KG).

- **Advantages**: The KG provides **symbolic, verifiable knowledge** that complements the model’s learned knowledge ([[2407.18470] Synergizing Knowledge Graphs with Large Language Models: A Comprehensive Review and Future Prospects](https://arxiv.org/abs/2407.18470#:~:text=,introduce%20a%20unifying%20framework%20designed)). It can enable multi-hop reasoning where each hop is a logical relationship rather than a guess ([The Synergy Between Knowledge Graphs and Large Language ...](https://www.bigdatawire.com/2024/05/01/the-synergy-between-knowledge-graphs-and-large-language-models/#:~:text=The%20Synergy%20Between%20Knowledge%20Graphs,single%20graph%20and%20facilitate)). For example, to answer “Which set introduced the most new Goblin cards?”, a KG query could traverse “Set -> contains cards -> filter Goblins -> count”. An LLM alone might hallucinate such specifics, but with a KG, we get precise data. Moreover, KGs can be updated incrementally (add new card nodes, new sets, ban list changes) without retraining the model. They also help interpretability: we can see the connections the model might be relying on.

- **Challenges**: Building and maintaining the KG is non-trivial. We’d need to ingest card data and meta knowledge into graph form, which requires schema design. Also, the model needs a mechanism to interface with the KG. Purely giving it as text might lose the structured benefit. A likely approach is to implement a **tool** or API that the model can call with a query (like “search KG for [cards with Flying in Standard]”) and then the tool returns results which the model then incorporates. Training the model to know when and how to call the KG is part of the “dynamic invocation” problem. Another challenge: not everything is easily captured in a KG (the comprehensive rules doesn’t translate well to SPO triples). We might restrict the KG to cards and simple rule flags (like attributes of cards, legalities, keywords relationships) – basically a card database with relationship edges, which is effectively what Gatherer or Scryfall’s database is (we can think of those as a graph accessible via queries).

### Retrieval-Augmented Generation (RAG)

RAG involves connecting the LLM with an unstructured knowledge corpus via retrieval. We will prepare a **vector database** of documents (or passages) that the model can query based on the user’s question, then feed the retrieved text into the model’s context so it can ground its answer. In our MTG project, RAG will be a cornerstone, especially for rules and card text:

- **Knowledge Sources for RAG**:

  - **Comprehensive Rules**: Split the rules document into sections or individual rules (indexed by their rule number and text). Each rule or cluster of related rules becomes a retrievable chunk. For example, if a question involves _replacement effects_, the engine should retrieve the rule section on replacement effects.
  - **Card Oracle Texts**: Each card’s text (and possibly rulings) can be a document. Queries containing a card name can directly retrieve that card’s official text. This ensures the model always has the **exact wording** of cards rather than relying on memory (which could be faulty or outdated).
  - **MTG Card Glossary / Keywords**: Documents explaining keywords (Flying, Trample, etc.), as well as any gatherer FAQ on mechanics.
  - **Deck and Meta Info**: We can also store tournament results summaries, meta reports, or deck guides as documents. If a user asks “What’s a good deck in Modern right now?”, the retriever could fetch a paragraph from a recent meta article and the model can draw on it. Similarly, if they ask details about a specific deck archetype, an article on that archetype can be provided.
  - **Community Q&A archives**: Potentially, gather prior QA (like StackExchange answers or judge forum answers) as documents. Then if a similar question is asked, the model can retrieve the answer and paraphrase. This could greatly boost accuracy on obscure interactions.

- **Integrating RAG**: The system will use the user’s query to perform a semantic search in these knowledge bases. The top relevant snippets (e.g., the relevant rule and the relevant card texts) will be appended to the model’s prompt as context. We’ll format it clearly, e.g.:

  **User question:** “Can I Counterspell an ability like that of Aether Vial?”  
  **Retrieved context (hidden from user or shown as citation):**

  - Rule 112.7a: Only spells (cards on the stack) can be targeted by spells or abilities that counter spells. Activated or triggered abilities on the stack are not spells.
  - Card text of Aether Vial (for reference).

  **Model answer:** “No, **Counterspell** cannot target an activated ability. Counterspell specifically counters a spell, and abilities are not spells on the stack【... rule reference ...】. You’d need something like Stifle to counter an ability.”

  By grounding the answer in retrieved text, we improve factual accuracy and reduce hallucinations ([Retrieval augmented generation: Keeping LLMs relevant and current](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%3A%20Keeping%20LLMs,date%20training%20data)). The model basically uses the provided text as an extension of its knowledge. This is critical for an authoritative rules answer — we want it to essentially quote the rule or card text to be sure.

- **Advantages**: RAG keeps the model **relevant and current** ([Retrieval augmented generation: Keeping LLMs relevant and current](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/#:~:text=Retrieval%20augmented%20generation%3A%20Keeping%20LLMs,date%20training%20data)). New card set released? Just add the new card texts to the index and the model can answer questions about them. No retraining required. It also massively cuts down on required memorization; the model doesn’t have to have every niche rule or card detail in its weights, as it can pull them on the fly. This also helps prevent hallucination of card details or rule numbers because it will rely on actual documents for those. The presence of context can also be used to have the model output sources or citations, increasing user trust (though format of answer will depend on design – might not always cite verbatim, but at least the content will be correct).

- **Challenges**: RAG requires a robust retrieval system. We must ensure high recall for relevant info – using good embeddings, perhaps hybrid search (some keyword matching for card names). If the retriever fails to find the needed info, the model might resort to guessing (hallucinating). To mitigate this, we’ll design fallback behaviors: if the model _thinks_ a retrieval is needed but none was provided, it might respond with uncertainty or ask for clarification rather than guessing. Another challenge is **prompt length** – with 32k context in Mixtral, we have room to include a lot of retrieved text, but we should still be concise. We might need to truncate or select the most crucial parts of rules. Summarization of retrieved text is an option (the model itself can do it). We also have to integrate this in real-time: it implies our system architecture will include a retriever component in the inference loop (see Architecture section).

### Combining KG and RAG, Dynamic Invocation

The ultimate system will have both a structured KG and an unstructured document store. The model (or a controller) should decide which to use for a given query. **Trade-offs and synergies**:

- The **KG** is best for questions that are fundamentally about relationships or lists of cards (structured queries). For example, “List all one-mana counterspells in Modern” – the KG (with card data) can directly give that list accurately, whereas the model might forget one if it relies on memory. The model could translate the user question into a graph query, get the results, then present them. Similarly, “What decks are built around card X?” could be answered by traversing an archetype graph linking X to deck archetypes. KG usage ensures completeness and precision.

- **RAG** is best for explanatory or rules queries where some descriptive text is needed. For instance, “How does banding work?” – retrieving the comprehensive rules section on banding will give the exact (and complicated) explanation which the model can then simplify. Or “What’s the current meta in Pioneer?” – retrieving a recent article will yield a rich description that the model can digest and summarize.

- There is overlap: e.g., a question like “Name a combo with Card A and Card B” could be approached via KG (if the combo is recorded as a relation) or via RAG (maybe an article mentions that combo). In such cases, we might use both: retrieve any known combos from the KG and also search articles for those card names together.

**Dynamic Invocation**: We want the system to invoke KG or RAG _only when needed_. Two possible designs:

1. **System-driven retrieval**: Use a simple classifier or heuristic on the user query. For example, if the query contains a specific card name or rule-like language, always retrieve relevant text (RAG). If the query asks for a list/count or has keywords like “list, which cards, how many”, use the KG to fetch structured data. This approach keeps the model simpler (it just gets additional context without explicitly “deciding” itself). We can refine these rules with experience.
2. **Model-driven (Agent) retrieval**: Train the model to act as an agent that can issue tool calls. In the prompt, we provide a format or instructions that it can output a special token or command like `<KGQuery>...</KGQuery>` or `<Search>...</Search>` when it needs information. If the model outputs such a token sequence, our system will intercept it, perform the query, and then return the results (possibly as a system message or appended to context), and then the model continues. This is the more flexible approach and aligns with techniques from _ReAct (Reason+Act)_ prompting and Toolformer. For example, the model’s chain-of-thought might be: “I recall not all counterspells can hit abilities. I should check the rules about countering abilities.” Then it outputs `<Search query="counter activated ability rule">`. The system does RAG retrieval, finds the rule, feeds it back, and the model then says, “According to rule 112.7a...”. Training a model to do this reliably would require some supervised examples of tool use. We could fine-tune it on a small “agent” dataset where it’s shown using a tool appropriately (some open datasets exist for LLM tool use, or we create a few examples manually). Given the complexity, a hybrid approach might suffice initially: some automatic retrieval plus the model trained to expect context if available and to use it.

- **Synergy**: KGs and RAG can even work together. For instance, the model could use the KG to retrieve an identifier or list (like a list of card IDs that satisfy some condition) and then use that to fetch expanded info via RAG (like retrieving those cards’ details from text). However, in our scope, we can treat them as separate tools. One interesting synergy: the KG can store identifiers for documents. E.g., a card node in the KG could link to its Oracle text document in the retriever. So if the model finds a card node via KG, it can easily get the text by a secondary retrieval step. We will ensure that the KG and document store share identifiers (like card name keys) to facilitate this.

**Best Practices:** We’ll follow best practices such as:

- Keep the retrieved context **concise and relevant**, perhaps by including only the sections that likely answer the query (to avoid diluting the model’s focus).
- Use **citing** or attribution in answers when appropriate. Since authoritative answers are needed, the model might say “(Comprehensive Rules 112.7a)” to back up an answer. This can be encouraged by examples in fine-tuning.
- Ensure the model knows the _limits of its knowledge_. If a question is about a brand new card or something not in its training data, it should naturally rely on retrieval. We can fine-tune it to say “According to the latest data I have... [info].” If something isn’t found, it should be comfortable saying it’s not certain.

In conclusion, by leveraging **KG for structured queries** and **RAG for unstructured knowledge**, we cover all knowledge needs: the KG provides _symbolic precision_ and the RAG provides _rich detail_. Combining them addresses the deficiencies of pure LLM memory (which might be inconsistent or outdated) with reliable data sources ([[2407.18470] Synergizing Knowledge Graphs with Large Language Models: A Comprehensive Review and Future Prospects](https://arxiv.org/abs/2407.18470#:~:text=,introduce%20a%20unifying%20framework%20designed)). Our architecture will incorporate both and allow either the system or the model to invoke them as needed to achieve the best of both worlds. This approach ensures that the LLM’s impressive reasoning ability is grounded in _true_ MTG knowledge, minimizing mistakes and hallucinations.

## System Architecture & Scalability

To implement the above solution, we need a robust system architecture that supports training, inference (with KG/RAG integration), and deployment, with scalability in mind. Below is a high-level outline of the system’s components and how they interact:

### Components Overview

- **Data Pipeline**: A subsystem for gathering, storing, and preprocessing all training data. This includes scrapers for online resources (decklists, forums), tools to parse and format game logs or rule documents, and a database or file storage for the curated datasets. The data pipeline will also handle the creation of the knowledge bases: building the vector index for RAG (embedding each rules section, card text, etc.) and constructing the knowledge graph (nodes, edges from our curated data). Scalability here means modular design – we can add new data sources (like ingest a new set’s cards) easily without rebuilding everything. Possibly use workflow managers or data version control to keep track of dataset versions as we refine them.

- **Model Training Environment**: We will utilize a **distributed training setup** that can accommodate fine-tuning Mixtral 8x7B with QLoRA. Initially, a single high-memory GPU (e.g., 48GB A6000 or 80GB A100) could suffice thanks to 4-bit quantization ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=,new%20data%20type%20that%20is)). For faster training or larger experiments, we can scale to multi-GPU: since Mixtral is effectively ~40B parameters, two 24GB GPUs in parallel could host it (each expert on one, for example) or 4 GPUs with tensor parallelism if needed. Using frameworks like Hugging Face Transformers + PEFT for QLoRA, and potentially DeepSpeed or PyTorch FSDP, we ensure we can scale the fine-tuning process as data grows or if we try a bigger base model in the future. We also plan to use experiment tracking (Weights & Biases or similar) to monitor training runs, which aids in scaling experiments and comparing multiple runs.

- **Inference Engine (LLM + Tools)**: This is the core that serves answers to user queries. It comprises:

  - The **LLM** model (Mixtral 8x7B fine-tuned) loaded with optimized inference tooling (e.g., using bitsandbytes 4-bit for GPU memory efficiency, and FlashAttention for speed). Initially, one instance of the model might run on a single GPU, handling one query at a time (with streaming output possibly). To scale concurrent users, we can run multiple model instances across GPUs or use model parallel serving if one query itself needs multiple GPUs for speed.
  - The **Retriever** for RAG. Likely a separate service or library (like Faiss or ElasticSearch for vector search) containing our indexed documents (rules, cards, etc.). On receiving a query (or a sub-query from the model), it returns the top relevant texts. This needs to be fast (a few hundred milliseconds) to not bottleneck the system. If necessary, we will keep the embeddings model and index in RAM and use approximate nearest neighbor search for speed.
  - The **Knowledge Graph Database/API**. We can use a graph database (Neo4j, or even simpler an in-memory networkx if small) that can answer queries. For simple lookups (like all cards with property X), a well-indexed relational DB might suffice too. But a graph DB gives flexibility for multi-hop queries. We will build an API layer so that either the model (via a tool call) or the system (via predefined triggers) can query the KG. This API might take structured queries (like a Cypher query or a custom function call) and return JSON or text results.
  - The **Controller** (or Orchestrator). This logic sits between the user and the model. Its job is to interpret the user input, decide if KG or RAG should be invoked (and do so accordingly), assemble the final prompt for the model (including any retrieved context or tool results in a format the model understands), and then pass it to the LLM for completion. After the model produces an answer (and possibly reasoning), the controller can also format the output (e.g., remove internal reasoning if not to be shown, or attach citations if we want to show sources). In agentic mode, the controller also manages the loop of model tool calls: if the model outputs a `<Search>` command, the controller executes it, feeds results back, etc., until the model signals it has a final answer.
  - Optionally, a **Memory Store** for context if we want the system to remember earlier conversation history or user preferences (this could be as simple as including the conversation in the prompt, made feasible by 32k context). This is more relevant if the interface is conversational (which it likely will be, at least in a chat-like interface for users).

- **Frontend / Interface**: The user-facing part could be a web application or API. For example, a web UI that allows a user to input a question or game state (maybe even upload a screenshot or select cards from a list) and then see the model’s answer and reasoning. This interface will call the backend (controller + model) and display results. We should design it to handle the different modes: maybe a user can toggle “show reasoning” or “explain step-by-step” to explicitly invoke those modes. The interface might also support multi-turn conversation (especially for teaching or follow-up questions). Scalability here means possibly having multiple instances behind a load balancer if many users, and ensuring efficient use of model instances (e.g., if one model can handle N parallel threads of generation through prompt batching or if not, then scaling horizontally by running multiple copies).

- **Monitoring & Logging**: A subsystem to log queries, model responses, tool usage, and performance metrics. This is important for evaluation and for safety (we might want to ensure no incorrect info is given without notice). It will also help identify gaps in knowledge to refine training. Over time, logs of real queries can be used to further fine-tune the model (with caution, ensuring we correct any mistakes in responses before using them as training).

### High-Level Flow (Inference)

1. **User Query** arrives (e.g., “Given this board state [...], what’s the best move?” or “Suggest a Commander deck around Card X” or a straightforward “What happens if ...?”).
2. **Controller** processes the query:
   - If the query contains keywords or patterns that match certain triggers, it decides to use KG or RAG. For example, it detects card names, so it queries the card text via RAG; or it sees a “list” request, so it queries the KG for a list. It may also rephrase or simplify the query for the tools (especially for search – e.g., remove fluff words for better retrieval).
   - It performs **Retrieval**: calls the vector search on our knowledge base. Suppose the query mentions “Aether Vial” and “Counterspell”; the search might retrieve the Comprehensive Rule on countering abilities and Aether Vial’s oracle text. It performs **KG lookup** if needed: maybe the query was “What sets have Aether Vial?” so it finds the card node and gets linked set nodes.
   - The controller then **assembles the prompt**: This could be a system message that provides context like “You are MTG-Guru, an AI that answers Magic: The Gathering questions with expert precision.” Then context: “Context: [insert retrieved text snippets or data].” Then the user’s question as the final prompt portion. We will have designed the prompt format during fine-tuning so the model expects, for instance, a section labeled “Context” that it should use but not repeat verbatim in the answer. If the model is agentic, the controller might instead feed the question and let the model ask for context (in which case steps happen in a loop).
3. **LLM Model** generates a response based on the prompt. Thanks to fine-tuning and the provided context, it should produce a high-quality answer, possibly with reasoning steps. If our design is to hide reasoning from the final answer, the model might output it internally (or we do a second pass where the model explains if user asks). In a simple case, it just answers the question with any necessary explanation, using the retrieved info to ground it.
4. **Post-processing**: The controller may strip out any special tokens or add formatting. For example, if we had the model include citations placeholders, the controller could replace them with actual source references. If the model’s answer is the recommended move, and we have the reasoning hidden, the controller might attach a “The model reasoned about this and determined the above move” note if needed.
5. **Response to User**: The answer is returned to the user on the interface. If it’s a decklist, it could be nicely formatted; if it’s a rules answer, maybe sources are shown. The interface can allow thumbs-up/down feedback which we feed to the monitoring system.

### Scalability Considerations

- **Horizontal Scaling**: Each model instance can handle one conversation at a time (to maintain context). If we have multiple simultaneous users, we will run multiple instances (on separate GPUs or via time-slicing if one GPU can schedule them, although latency might suffer). The architecture should be stateless between requests aside from conversation context, meaning we can route any user’s next query to the same instance if context is stored, or we encode the context in the prompt itself. A load balancer can distribute incoming requests to free workers. We should containerize the inference service (Docker with the model and retrieval components) so it’s easy to spin up replicas on additional nodes as needed.
- **Model Upgrades**: If in the future we obtain more powerful hardware, we might fine-tune a larger model (say a 34B or 70B dense model, or if Mistral releases a 16x7B MoE). The architecture is designed such that the model is somewhat plug-and-play – the surrounding retrieval and KG components remain the same. We’d retrain or swap in the new model, and because we used standard interfaces (like the prompt format), minimal changes are needed. We also isolate model-specific code (the HuggingFace model loading, etc.) so that we can deploy a new model version in parallel and do A/B testing if desired.
- **Data & Knowledge Scaling**: As more data (cards, rules updates, etc.) come in, the KG and vector index need updating. We will schedule periodic re-indexing (like when a new card set is released, run a script to add those cards to both KG and RAG store). The retrieval search might slow slightly as data grows, so we ensure our vector search is efficient (maybe moving to ANN with HNSW that scales sub-linearly). Also, if we start supporting multi-lingual queries (since Mistral supports other languages ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=Mistral,than%20Mixtral%208x7B))), we might need multi-lingual knowledge or separate indexes per language. Initially, we focus on English.
- **Future Features**: If later on, we decide to integrate other modalities (like analyzing a screenshot of a board state via a vision model or parsing text from it), the architecture can accommodate that by adding a pre-processing step (OCR or vision encoder that turns an image into a text description for the LLM). We mention this to illustrate that the modular design (separate components for each function) allows extension: e.g., plug in a “image2state” module before the controller.
- **Security and Robustness**: We should sandbox the model’s tool usage (the controller will have strict allowed operations for KG queries to avoid any injection or malicious queries). And the vector index should be read-only from the model’s perspective (to avoid the model trying to inject or alter knowledge – unlikely but a consideration). The system will be built and tested thoroughly to ensure it handles malformed inputs gracefully (maybe the model responds with a clarifying question if it doesn’t understand).

This architecture ensures that training and inference are well-separated but complementary. It also ensures that as usage grows or as our knowledge base grows, we can scale horizontally (more instances) or vertically (bigger model, more data) without redesigning from scratch. In essence, we are building an **LLM-powered MTG expert system** with a solid engineering backbone to support it in production.

## Project Plan (Roadmap)

Developing this MTG-savvy LLM will be an iterative process. We outline a phased roadmap with milestones to ensure structured progress from inception to deployment:

**Phase 0: Feasibility & Setup**

- _Goal_: Verify that the concept is viable on a small scale and set up infrastructure.
- **0.1:** _Literature Review & Prototyping_ – Research existing models and techniques (some of which is reflected in this analysis). Possibly fine-tune a tiny model (like a 1-3B model) on a very limited MTG dataset as a proof-of-concept to see how it performs on basic tasks. Experiment with the retrieval pipeline on a simple Q&A.
- **0.2:** _Environment Setup_ – Acquire necessary hardware or cloud instances. Set up training frameworks (PyTorch, Transformers, PEFT), and retrieval systems (installing a vector DB, etc.). Prepare repositories and version control for code and data.

**Phase 1: Data Collection & Preparation**

- _Goal_: Build the comprehensive MTG dataset and knowledge bases.
- **1.1:** _Gather Game Data_ – Collect sources for gameplay scenarios. This might involve web scraping or manual input from community puzzles. Clean and format these into a consistent structure (likely JSON or CSV with fields for state description, optimal move, explanation).
- **1.2:** _Deck Data Acquisition_ – Scrape or download deck lists and archetype info. For example, use the MTGGoldfish API or website to get top decks for each format. Also scrape deck guide articles. Organize by format and archetype.
- **1.3:** _Rules & Card Data_ – Download the Comprehensive Rules (which might be available as a text or PDF from Wizards). Parse it into sections. Also fetch the Oracle card database (gatherer or Scryfall provides bulk data JSON). Extract card name, text, and rulings. Build our knowledge graph structure from this (this can be done in code, generating nodes and edges lists). Index all textual data for retrieval (e.g., use a sentence-transformer to embed).
- **1.4:** _Community Q&A and Tutorials_ – If allowed, scrape Q&A from forums or StackExchange, and gather educational content (maybe from MTG wiki or YouTube transcripts of tutorials). These augment the _TEACH_ mode training.
- **1.5:** _Data Cleaning & Annotation_ – Go through collected data to ensure quality. Deduplicate similar questions, remove outdated rule references, standardize card names (e.g., ensure card names are spelled exactly as in Oracle). Possibly annotate certain data with tags (like which reasoning mode it exemplifies) for use in fine-tuning (we can condition on these tags if needed).
- **1.6:** _Dataset Assembly_ – Split into training, validation, and test sets. The test set should include some tough cases curated by domain experts (maybe a set of “championship puzzles” or tricky rule questions withheld for final evaluation). Freeze this split to evaluate model improvements consistently.
- _Deliverable_: A well-structured dataset (or set of datasets) ready for fine-tuning, and initial versions of the KG and document index. We should also have a document describing the dataset (for record and potential open-sourcing if possible).

**Phase 2: Initial Fine-Tuning**

- _Goal_: Fine-tune the base model on the dataset to get a first usable version of the MTG-LLM.
- **2.1:** _Base Model Preparation_ – Load Mixtral 8x7B (or chosen model) in our training setup. Test it on a sample input to confirm everything works (since MoE might need special loading). Apply QLoRA preparation (quantize model to 4-bit, attach LoRA adapters).
- **2.2:** _Training Run 1 (Core Fine-tune)_ – Train on the combined dataset (or sequentially as planned). Use a moderate learning rate and monitor loss on validation set. Because this is multi-task, watch each subset’s metrics if possible (we might have some preliminary automatic metrics, like % of rules questions answered correctly on val set, etc.). Training may run for several epochs – we’ll likely use early stopping if no improvement on val. This stage yields the first fine-tuned model (v1).
- **2.3:** _Evaluate & Debug v1_ – Rigorously test the model on the validation and test sets. Identify areas of weakness. For example, maybe it’s doing well on rules but deck suggestions are too generic, or it’s good at simple states but fails on complex boards. Gather these findings. Also test in an interactive setting (a small script to ask it questions as a user would). If possible, have an MTG expert or two try some queries and give feedback.
- **2.4:** _Refinement Training_ – Based on v1 evaluation, prepare additional training data for problem areas. This may involve generating new examples or oversampling parts of data. For instance, if it struggles with _RETROSPECT_ questions, add more of those and train for a few more epochs (possibly as a separate LoRA merge or just continuing training). Also, incorporate any new data that became available (maybe we didn’t have time in Phase 1 for everything). Train to get v2. We might do a few cycles of this on specific sub-tasks (this is somewhat like curriculum learning – focus on one skill at a time if needed).
- **2.5:** _Incorporate R1-style Training_ – At this point, the model should be decent at following instructions and basic reasoning. We can now incorporate the “advanced reasoning” finetuning. For example, fine-tune on a small dataset that specifically includes chain-of-thought format (with the special tokens) so the model learns to output those. Or do a run with a lower learning rate on the Dolphin-R1 dataset ([README.md · mlabonne/dolphin-r1-deepseek at main](https://huggingface.co/datasets/mlabonne/dolphin-r1-deepseek/blob/main/README.md#:~:text=This%20is%20a%20reformatted%20version,end_of_solution)) to imbue that style. This could yield model version v3 (with strong reasoning consistency). We’ll have to check that it still behaves well on user queries (doesn’t confuse users with too much internal thought unless asked). Ideally, the model now has the capability to produce a hidden reasoning trace and a final answer – which we can utilize or suppress in deployment.
- _Deliverable_: A fine-tuned LLM (with LoRA adapters) that has shown significant proficiency in all target areas during testing. We should also have logs/metrics demonstrating its performance (like it got X% of test scenarios correct, or judges rated its answers 8/10 on average, etc.).

**Phase 3: Tool Integration (KG & RAG)**

- _Goal_: Connect the fine-tuned model with the external knowledge sources and test the combined system.
- **3.1:** _Implement Retrieval Pipeline_ – Set up the vector database with a simple API. Write the code to take a query, embed it (likely using the same model or a smaller one if needed), perform similarity search, and return top-k texts. Test this independently with sample queries to ensure it finds relevant snippets (e.g., test query “Banding damage assignment” returns the banding rule text). Fine-tune or adjust embeddings if results aren’t relevant (maybe add some keyword weighting or use a hybrid search approach).
- **3.2:** _Implement Knowledge Graph Queries_ – Stand up the graph database (or a Python-based graph query module). Populate it with our node-edge data. Write a few query functions for common needs (like `get_cards_by_property(property, format)`, `get_deck_archetypes_for_card(card)`, etc.). Also allow a raw query mode if needed. Test queries on known relationships (like ensure “Birds of Paradise” node shows it’s a creature with mana ability, etc.).
- **3.3:** _Orchestrator Logic_ – Develop the controller that will orchestrate model and tools. At first, this can be rule-based: e.g., if query contains “legal” or “format”, hit the KG for legality info; if contains known card name, retrieve that card text; always retrieve relevant rule text for any rules-like query. Implement these and log what it decides for various test queries.
- **3.4:** _Model Interface for Tools_ – Since our model was fine-tuned mostly on static Q&A, we might need to do a bit more fine-tuning or at least prompt engineering to teach it to use provided context. For example, when we supply “Context:” in the prompt, ensure the model knows to use it. We can do a small _prompt tuning_ here: craft a few example prompts with context and expected answers and see if the model follows. If it struggles to properly incorporate context (some models might ignore or hallucinate beyond provided info), we may fine-tune on a few examples of using context. Similarly, if we want the model to output tool commands itself, we’d definitely fine-tune some examples of that format. This could be Phase 2.5 as well (integrated earlier).
- **3.5:** _Integration Testing_ – Now run end-to-end tests: simulate various user questions through the full system. For example, ask a rules question: controller fetches rule, prompt goes into model, model answers. Check if the answer used the context correctly and is accurate. Do this for each type of query (deck building, gameplay scenario, etc.). This might reveal issues like the model ignoring KG data or the controller misidentifying a query type. We refine accordingly (adjust rules or fix edge cases in code).
- **3.6:** _Feedback Loop Implementation_ – (Optional but good) Set up a mechanism for the model to ask for more info if needed. For instance, if no context was given but the model is unsure, maybe it responds with “I need more information about X to answer”. While fine-tuning, we might have allowed it to say “I’m not sure”. In deployment, perhaps we capture that and trigger a broader search or even ask the user a follow-up. This is more of a feature enhancement, can be considered if time permits.
- _Deliverable_: A working **MTG LLM Assistant** system where the model and knowledge sources work in concert. At this stage, we likely have a backend server that can accept requests and produce answers correctly for test queries.

**Phase 4: Evaluation & Iteration**

- _Goal_: Rigorously evaluate the system’s performance and improve it with final tweaks.
- **4.1:** _Quantitative Evaluation_: Using our test set (which might include 100+ varied questions covering all capabilities), evaluate the model’s answers. This could be done by humans rating each answer for correctness and helpfulness. For rules questions, check against official answers; for gameplay, have expert players see if the recommended move is sensible; for deck suggestions, perhaps compare with known good decklists or ask if the card choices align with the archetype. We might also use some automated metrics: e.g., accuracy on a set of multi-choice rules questions, or a BLEU score against reference explanations (though these tasks are complex for simple metrics).
- **4.2:** _Qualitative Evaluation_: Invite a few MTG players of varying skill to interact with the model (via a chat UI) and give feedback. They can ask it to analyze their deck, or pose a tricky scenario. Gather their impressions: Did it make logical sense? Was any explanation confusing? Did it handle follow-up questions? This will highlight any usability issues or any tendency to occasionally produce wrong answers confidently (which we need to minimize).
- **4.3:** _Safety and Consistency Check_: Ensure the model doesn’t produce disallowed content. For example, MTG is generally safe content, but ensure it doesn’t deviate into harassment or unrelated things if prompted weirdly. Also, check consistency: ask the same question phrased differently to see if answers remain consistent (especially with retrieval in play, it should). If not, adjust either the retrieval or possibly fine-tune for consistency (maybe by always citing source to ground answers).
- **4.4:** _Final Refinement_: Based on evaluation, do one more training tweak if needed. For instance, if it was found that the model occasionally gives a wrong rules answer because it misunderstood context, we fine-tune on that case explicitly or adjust the prompt template to emphasize using provided text. Or if deck suggestions missed obvious choices, perhaps feed more deck data or even hard-code the KG with those associations. This is the polish stage to handle any glaring problems.
- **4.5:** _Benchmarking_: If relevant, benchmark the model on known LLM benchmarks just to see relative performance (e.g., see how it scores on a general reasoning set or a domain-specific test if one exists for MTG). This is secondary, but good for documentation.
- _Deliverable_: Evaluation report showing the model’s capabilities (maybe “achieves 90% accuracy on rules questions, wins X% of simulated games against baseline, etc.” or at least that it meets the project goals qualitatively). This phase ensures we are confident in the model’s quality before wide release.

**Phase 5: Deployment**

- _Goal_: Deploy the system for real users (perhaps as a web app or API).
- **5.1:** _Documentation & README_: Finalize documentation for the project – including the README (see next section) that introduces the project in a thematic way, and technical docs on how to run the system. Clearly explain how to start the server, how to update the knowledge bases, etc., so developers or the devOps team can maintain it.
- **5.2:** _Web Interface Implementation_: Build/finish the user interface. Should be intuitive: maybe a chat-like interface for asking questions. Possibly specialized UIs for specific tasks (like a form where you input a board state in a structured way for analysis, or a deck builder UI where as you select archetype it fills in suggestions). Initially, a simple chat text box might suffice, with maybe a dropdown for “mode” (analysis, deck, etc. which simply helps format the prompt). Use a modern web framework or even a Jupyter notebook interface for early demo, then move to a proper web app.
- **5.3:** _Server Deployment_: Host the model and services on a production server or cloud service. Ensure the environment has the required GPU resources. Dockerize the application for consistency from dev to prod. Run load tests if expecting many users to ensure it can handle concurrent queries (e.g., simulate 10 users asking at once, see that latency is okay with multiple model replicas). Optimize if needed (maybe use 8-bit inference if 4-bit was for training; or use batch processing if low latency isn’t critical, etc.).
- **5.4:** _Monitoring in Production_: Set up monitoring on the live system – track usage, latency, any errors, and collect user feedback or ratings if available. This will help catch any issues early (like memory leaks or certain questions causing the model to hang).
- **5.5:** _Launch & Announcement_: Once stable, release the project to the intended audience. If open-source, publish the model weights (LoRA adapters likely) and code, along with the project README that sets the tone. Encourage users to try it and report issues or share interesting use cases.
- **5.6:** _Post-Launch Iteration_: Be ready to fix any bugs (hot-fix the controller rules if a pattern of problematic queries appears). Perhaps schedule periodic re-training if a lot of new data (like new card sets) have come out – or rely on the retrieval to handle those seamlessly.

- _Deliverable_: A deployed, accessible MTG LLM assistant, project documentation, and a plan for maintenance.

**Phase 6: Future Enhancements** (beyond the scope of initial project, but noted for vision)

- Integrate a game engine to actually play games or validate suggested plays via simulation.
- Expand to other languages (since many MTG players speak Spanish, Japanese, etc., it would be great to answer rules in those languages — Mistral being multilingual helps ([Mixtral 8x7B: A game-changing AI model by Mistral AI | SuperAnnotate](https://www.superannotate.com/blog/mistral-ai-mixtral-of-experts#:~:text=Mistral,than%20Mixtral%208x7B))). This could involve translating the knowledge base and fine-tuning on non-English data.
- Explore a smaller distilled model for mobile use, etc., using our fine-tuned model to distill down to a 13B or 7B model for lightweight usage.
- Multi-modal input: allow uploading a photo of a board or an Arena game log for analysis directly, which would really showcase the AI’s utility.

The roadmap above ensures a structured progression. Each phase builds towards the final goal, with evaluation checkpoints to ensure we’re on track. By the end of Phase 5, we should have a named product – an AI _Planeswalker_ of sorts – ready to assist players and enthusiasts.
